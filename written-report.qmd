---
title: "Factors Impacting Number of LinkedIn Job Applications per Post View"
author: "JRLK - Jessie Ringness, Rebekah Kim, Laura Cai, Karen Dong"
date: 11/14/2023
format: pdf
execute: 
  warning: false
  message: false
editor: visual
---

```{r}
#| label: load-pkg-data
#| message: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(knitr)
library(patchwork)
library(janitor)
library(ggplot2)
library(patchwork)
job_postings <-read.csv("data/job_postings.csv")
benefits <-read.csv("data/benefits.csv")
employee <-read.csv("data/employee_counts.csv")
```

```{r manipulating_benefits}
#| echo: false
benefits <- benefits |>
  select(-inferred) |>
  mutate(count = 1) |>
  pivot_wider(names_from = "type", values_from = "count") |>
  clean_names()
```

```{r}
#| echo: false
benefits <- replace(benefits, is.na(benefits), 0)
benefits$tot_benefits <- rowSums(benefits == 1)
benefits_total <- benefits[, c('job_id', 'tot_benefits')]
```

```{r joining_jobs_employee}
#| echo: false
jobs_employee <- job_postings |>
  left_join(employee, by = join_by("company_id"))
```

```{r joining_final_set}
#| echo: false
linkedin <- jobs_employee |>
  left_join(benefits_total, by = join_by("job_id"))

linkedin <- linkedin |> distinct(job_id, .keep_all = TRUE)
```

```{r pay_period_mutations}
#| echo: false
linkedin <- linkedin |>
  mutate(pay_period = recode(pay_period, YEARLY = 1/2080, HOURLY = 1, MONTHLY = 1/160)) |>
  mutate(hourly_max_salary = pay_period * max_salary) |>
  mutate(if_benefits = factor(ifelse(linkedin$tot_benefits > 0, "listed", "none"),
  levels = c("none", "listed")))
linkedin <- linkedin |>
  mutate(per_applies = applies/views) 
```

```{r time_drop_na_mutations}
#| echo: false
linkedin_subset <- linkedin |>
  mutate(original_listed_time = as.POSIXct(original_listed_time/1000, origin = "1970-01-01", tz = "EST")) |>
  mutate(listed_time = as.POSIXct(listed_time/1000, origin = "1970-01-01", tz = "EST")) |>
  drop_na(hourly_max_salary) |>
  drop_na(per_applies) |>
  drop_na(follower_count) |>
  drop_na(formatted_experience_level) |>
  drop_na(remote_allowed) |>
  drop_na(original_listed_time)
```

```{r orig_hour_mutations}
#| echo: false
linkedin_subset$hour_listed <- sapply(strsplit(as.character(linkedin_subset$original_listed_time), ' '), function(x) {
  time_parts <- unlist(strsplit(x[2], ':'))
  return(time_parts[1])
})
```

```{r numerical_plots}
#| echo: false
salary_hist <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary)) + 
  geom_histogram()
follower_hist <- linkedin_subset |>
  ggplot(aes(x = follower_count)) + 
  geom_histogram()
hr_hist <- linkedin_subset |>
  ggplot(aes(x = as.numeric(hour_listed))) + 
  geom_histogram()

(salary_hist + hr_hist) / (follower_hist)
```

```{r }
#| echo: false
experience_plot <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary, y = per_applies, color = formatted_experience_level)) +
  geom_point() + 
  geom_smooth(method = 'lm',formula = y ~ x, se = F) +
  labs(x = "Hourly Maximum Salary",
       y = "Applications per view",
       color = "Experience Level",
       title = "Applications per view based on 
       experience level and maximum salary") + 
  facet_wrap(~formatted_experience_level)
experience_plot 
```

```{r}
#| echo: false
salary_scat <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary, y = per_applies)) +
  geom_point()

follow_scat <- linkedin_subset |>
  filter(follower_count > 10000) |>
  ggplot(aes(x = follower_count, y = per_applies)) +
  geom_point()

hr_scat <- linkedin_subset |>
  ggplot(aes(x = hour_listed, y = per_applies)) +
  geom_point()

(follow_scat + salary_scat) / hr_scat
```

```{r}
#| echo: false
group <- linkedin_subset |>
  group_by(hour_listed) |>
  summarize(mean_per_applies = mean(per_applies))
```

```{r}
#| echo: false
group$hour = as.numeric(group$hour_listed)
```

```{r}
#| echo: false
group |>
  ggplot(aes(x = hour_listed, y = mean_per_applies)) +
  geom_col()
```

## Introduction

LinkedIn is a popular platform that connects companies and professionals spanning various levels of experience, and there are thousands of active job postings available on LinkedIn. Moreover, online job search services and platforms are now considered equally important for people to access a wide variety of opportunities compared to in-person job postings. With the sheer amount of postings, applicants may be overwhelmed by the vast amount of postings, and are less likely to come across some postings over others. Our primary research question is - what variables about job postings increase popularity among applicants?

This data set was created by Arsh Koneru-Ansari in July 2023, who used Python to scrape data directly from linkedin.com. The scraper code is published in their GitHub (<https://github.com/ArshKA/LinkedIn-Job-Scraper#jobs>).

The data dictionary for the variable definitions can be found in the ReadMe for the data. The variables we will focus on are:

-   **applies:** number of applications that have been submitted

-   **views:** number of times the job posting has been viewed

-   **max_salary:** maximum salary offered in the position 

-   **remote_allowed:** whether job permits remote work (1 = yes)

-   **follower_count:** number of company followers on LinkedIn

-   **listed_time:** time when the job was listed, in UNIX time

-   **formatted_experience_level:** job experience level (entry level, associate, mid-senior level, director, executive, internship) 

-   **type**: type of benefit provided (Medical insurance, Dental insurance, 401(k), Paid maternity leave, Disability insurance, Vision insurance, Tuition assistance, Pension plan, Child care support, Commuter benefits, Student loan assistance)

Another potential interaction effect is the number of views and experience level. Different positions are more sought after on LinkedIn over others, depending on viewers' backgrounds. The number of applications increases per number of views for a position requiring associate experience increases at a more rapid rate than those for other positions, although all positions have some sort of impact associated with views and applications.

## Methodology

While the bulk of our data was found in the \`job_postings\` data set, we also wanted to include employee count and follower count data in the \`employee\` data set and the type and number of benefits listed on the post found in the \`benefits\` data set. We needed to manipulate the data in \'benefits' to create a useful predictor as the majority of the data was NA, meaning no benefits could be found from the scraped data. To make the data from \`benefits\` a useful predictor, we created a new categorical variable called \`\'if_benefits\` where if a benefit (such as paid maternity leave or a 401k plan) was listed on the post, then the post was considered as having benefits \`listed\`, and otherwise was listed as \`none\` listed. We joined all data sets together by \`company_id\`, and saved the data set as \`linkedin\`. 

We also made some assumptions about other variables to normalize predictor variables. The categorical variable \`pay_period\` contained data on when the job would pay its worker the \`max_salary\` or \`min_salary\` amount, with hourly, monthly, and annual payments as the different levels. To normalize the \`max_salary\` amount, we calculated the hourly wage given the maximum pay for hourly, monthly, and yearly pay periods. We assumed 160 hours for the monthly payments (40 hour work week for 4 weeks), and 2080 hours for the annual payments (40 hour work week for 52 weeks). We saved the new data in a variable called \`hourly_max_salary\`. 

We then dropped all \`NA\` values for all predictors we wanted to observe so that we could keep our dataset consistent when testing different models, meaning NA values for \`hourly_max_salary\`, \`per_applies\`, \`follower_count\`, \`formatted_experience_level\`, \`original_listed_time\`, and \`remote_allowed\`. 

Because the number of views an application gets is directly related to the number of applications and the jobs have been listed for varying durations of time, we decided to normalize the number of applications with the views. To do so, we created a new variable \`per_applies\`. We then used \`per_applies\` as our response variable. 

Because \`per_applies\` is a numerical variable, a linear regression model would be most appropriate to predict the number of applications per view. As we addressed in the introduction, a person takes into consider many different factors when applying to a job, so our model takes into consideration multiple predictors, including the hour the job was posted, the number of followers the company has, the job experience level, the maximum salary, ability to work remote, and if benefits are listed. 

We split the \`linkedin\` dataset into training and testing data, with 75% of the data in training and 25% in testing. We then used cross-fold validation with 12 folds on the training data set to find the mean summary statistics (AIC,  BIC, Adjusted R-Squared) for each model and compared the different values to find the best possible model. This process was repeated for models containing each combination of predictor variables. We set a seed of (2) when splitting and folding the data to ensure reproducibility.

```{r split_data}
#| echo: false
set.seed(2)
linkedin_split <- initial_split(linkedin_subset)
linkedin_train <- training(linkedin_split)
linkedin_test <- testing(linkedin_split)
```

```{r cross_val}
#| echo: false
set.seed(2)
folds <- vfold_cv(linkedin_train, v = 12)
linkedin_spec <- linear_reg() |>
set_engine("lm")
```

```{r recipe_1}
#| echo: false
linkedin_recipe1 <- recipe(per_applies ~ job_id + hourly_max_salary + follower_count + remote_allowed + formatted_experience_level + if_benefits + hour_listed, data = linkedin_train) |>
  update_role(job_id, new_role = "ID") |> 
  step_center(hourly_max_salary, follower_count) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())
```

```{r workflow_1}
#| echo: false
linkedin_wflow1 <- workflow() |>
add_recipe(linkedin_recipe1) |>
add_model(linkedin_spec)
```

```{r}
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

```{r resample_mean_stats}
#| echo: false
linkedin_fit_rs1 <- linkedin_wflow1 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
collect_metrics(linkedin_fit_rs1, summarize = TRUE)
map_df(linkedin_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

The adjusted R^2 value for a model including `hourly_max_salary`, `follower_count`, `remote_allowed`, `formatted_experience_level`, `hour_listed`, and `if_benefits` was 0.0076. We removed variables one by one, but each model resulted in a lower R^2 value, indicating that a model including all mentioned variables is the best. 

```{r}
linkedin_fit <- linkedin_wflow1 |>
  fit(data = linkedin_test)
linkedin_test_pred <- predict(linkedin_fit, linkedin_test) |>
  bind_cols(linkedin_test)
rsq(linkedin_test_pred, truth = per_applies, estimate = .pred)
```

## Results

```{r}
linkedin_yr_rec <- recipe(per_applies ~ job_id + hourly_max_salary + follower_count + remote_allowed + formatted_experience_level + if_benefits + hour_listed, data = linkedin_subset) |> 
  update_role(job_id, new_role = "ID") |> 
  step_naomit(all_predictors()) |>
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors())
```

```{r}
#specify the model
linkedin_yr_spec <- linear_reg() |>
  set_engine("lm")

#build model workflow
linkedin_yr_workflow <- workflow() |>
  add_model(linkedin_yr_spec) |>
  add_recipe(linkedin_yr_rec) 

# fit the model 
linkedin_yr_fit <- linkedin_yr_workflow |>
  fit(data = linkedin_subset) 

tidy(linkedin_yr_fit) |>
  kable(digits = 3)
```

```{r}
glance(linkedin_yr_fit)
```


Looking at the how the maximum salary, follower count, whether the job allows remote, the experience level, if there were benefits listed, and the hour at which the job was listed all affected the ratio of applications to views, there is no significant relationship between these variables, since the adjusted R^2 s 0.0589, which is very low. This means that about 6% of the variation in the percent of viewers that applied is a result of the variation in the predictor variables we are studying, indicating no relationship.We tried different combinations of variables to see if there was any relationship between the variables with percent of viewers that applied.  

## Data dictionary

The data dictionary can be found [here](https://docs.google.com/document/d/15LOwqM0wwdBLOcGvvohmnEjVNeWslftThWEikwoEWNo/edit?usp=sharing).
