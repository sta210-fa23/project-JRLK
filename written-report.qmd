---
title: "Factors Impacting Number of LinkedIn Job Applications per Post View"
author: "JRLK - Jessie Ringness, Rebekah Kim, Laura Cai, Karen Dong"
date: 11/14/2023
format: pdf
execute: 
  warning: false
  message: false
editor: visual
---

```{r load_pkg_data}
#| message: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(knitr)
library(patchwork)
library(janitor)
library(ggplot2)
library(patchwork)
library(rms)
job_postings <-read.csv("data/job_postings.csv")
benefits <-read.csv("data/benefits.csv")
employee <-read.csv("data/employee_counts.csv")
```

```{r manipulating_benefits}
#| echo: false
benefits <- benefits |>
  select(-inferred) |>
  mutate(count = 1) |>
  pivot_wider(names_from = "type", values_from = "count") |>
  clean_names()
```

```{r benefits_manipulation}
#| echo: false
benefits <- replace(benefits, is.na(benefits), 0)
benefits$tot_benefits <- rowSums(benefits == 1)
benefits_total <- benefits[, c('job_id', 'tot_benefits')]
```

```{r joining_jobs_employee}
#| echo: false
jobs_employee <- job_postings |>
  left_join(employee, by = join_by("company_id"))
```

```{r joining_final_set}
#| echo: false
linkedin <- jobs_employee |>
  left_join(benefits_total, by = join_by("job_id"))
linkedin <- linkedin |> distinct(job_id, .keep_all = TRUE)
```

```{r replace}
#| echo: false
linkedin[["remote_allowed"]][is.na(linkedin[["remote_allowed"]])] <- 0
linkedin[["tot_benefits"]][is.na(linkedin[["tot_benefits"]])] <- 0
linkedin$remote_allowed <- as.factor(linkedin$remote_allowed)
```

```{r pay_period_mutations}
#| echo: false
linkedin <- linkedin |>
  mutate(per_applies = applies / views) 
linkedin_subset <- linkedin |>
  drop_na(max_salary) |>
  drop_na(pay_period) |>
  mutate(pay_period = recode(pay_period, YEARLY = 2080, HOURLY = 1, MONTHLY = 160)) |>
  mutate(hourly_max_salary = round((max_salary / pay_period), digits = 2))
```

```{r predictor_manipulations}
#| echo: false
linkedin_subset <- linkedin_subset |>
  mutate(original_listed_time = as.POSIXct(original_listed_time/1000, 
                                           origin = "1970-01-01", 
                                           tz = "EST")) |>
  mutate(listed_time = as.POSIXct(listed_time/1000, 
                                  origin = "1970-01-01", 
                                  tz = "EST")) 
linkedin_subset$hour_listed <- sapply(strsplit(as.character(linkedin_subset$original_listed_time), ' '), function(x) {
  time_parts <- unlist(strsplit(x[2], ':'))
  return(as.numeric(time_parts[1]))
})

linkedin_subset <- linkedin_subset |>
  mutate(if_benefits = factor(ifelse(linkedin_subset$tot_benefits > 0, "listed", "none"),
  levels = c("none", "listed"))) |>
  mutate(remote_allowed = if_else(remote_allowed == 1, "yes", "no")) |>
  mutate(time_posted = if_else(between(hour_listed, 0,5), "night", 
                               if_else(between(hour_listed, 6,11), "morning", 
                                       if_else(between(hour_listed, 12,17), "afternoon", 
                                               if_else(between(hour_listed, 18,23),"evening", NA)))))
```

```{r filtering_data}
#| echo: false
linkedin_subset <- linkedin_subset |> 
  filter(between(hourly_max_salary, 19.31, 114.04)) |>
  filter(between(follower_count, 328.15, 3326613.00)) |>
  filter(views>5)
```

```{r drop_vars}
#| echo: false
linkedin_subset <- subset(linkedin_subset, select = c(job_id,
                                            formatted_work_type,
                                            time_posted,
                                            formatted_experience_level,
                                            remote_allowed,
                                            work_type,
                                            employee_count,
                                            follower_count,
                                            per_applies,
                                            hourly_max_salary,
                                            if_benefits,
                                            hour_listed))
linkedin_subset <- linkedin_subset |>
  filter(formatted_experience_level != "") |>
  drop_na(c(job_id,
            formatted_work_type,
            time_posted,
            formatted_experience_level,
            remote_allowed,
            work_type,
            employee_count,
            follower_count,
            per_applies,
            hourly_max_salary,
            if_benefits,
            hour_listed))
```

## Introduction and data

LinkedIn is a popular platform that connects companies and professionals spanning various levels of experience, and there are thousands of active job postings available on LinkedIn. Moreover, online job search services and platforms are now considered equally important for people to access a wide variety of opportunities compared to in-person job postings. With the sheer amount of postings, applicants may be overwhelmed by the vast amount of postings, and are less likely to come across some postings over others. Our primary research question is - what variables about job postings increase popularity among applicants?

This data set was created by Arsh Koneru-Ansari in July 2023, who used Python to scrape data directly from linkedin.com. The scraper code is published in their [GitHub](https://github.com/ArshKA/LinkedIn-Job-Scraper#jobs).

The data dictionary for the variable definitions can be found in the ReadMe for the data. The variables we will focus on are:

-   **`applies`:** number of applications that have been submitted

-   **`views`:** number of times the job posting has been viewed

-   **`max_salary`:** maximum salary offered in the position 

-   **`remote_allowed`:** whether job permits remote work (1 = yes)

-   **`follower_count`:** number of company followers on LinkedIn

-   **`listed_time`:** time when the job was listed, in UNIX time

-   **`formatted_experience_level`:** job experience level (entry level, associate, mid-senior level, director, executive, internship) 

-   **`type`**: type of benefit provided (Medical insurance, Dental insurance, 401(k), Paid maternity leave, Disability insurance, Vision insurance, Tuition assistance, Pension plan, Child care support, Commuter benefits, Student loan assistance)

```{r numerical_plots}
#| echo: false
salary_hist <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary)) + 
  geom_histogram(fill = "cadetblue3") +
  labs(x = "Hourly Max Salary",
       y = "Jobs",
       title = "Max Salary Distribution")
follower_hist <- linkedin_subset |>
  ggplot(aes(x = follower_count)) + 
  geom_histogram(fill = "cadetblue") +
  labs(x = "Follower Count",
       y = "Jobs",
       title = "Follower Count Distribution")
hr_hist <- linkedin_subset |>
  ggplot(aes(x = time_posted)) + 
  geom_bar(fill = "cornflowerblue") +
  labs(x = "Time of Day Posted",
       y = "Jobs",
       title = "Job Postings Over Time")

(salary_hist + hr_hist) / (follower_hist)
```

```{r summary statistics}
#| echo: false
sal_quantile<-quantile(linkedin_subset$hourly_max_salary, probs = c(0.05, 0.5, 0.95), na.rm =TRUE)
follower_quantile<-quantile(linkedin_subset$follower_count, probs = c(0.05, 0.5, 0.95),na.rm =TRUE) 
views_quantile<-quantile(linkedin_subset$views, probs = c(0.1,0.5, 0.95), na.rm = TRUE)
```

```{r}
#| echo: false
linkedin_subset_small <- linkedin_subset[, c(3,8,10)]
summary_stats <- summary(linkedin_subset_small)

```

*Fig 1.1.* The distribution of hourly maximum salary is right-skewed with jobs in the data set having a generally lower hourly maximum salary. Given the apparent skewness the center is the median hourly maximum salary of around \$50 per hour. The IQR describing the spread of the 50% of the distribution is \$33 per hour, demonstrating that the variability of the hourly maximum is relatively high. The histogram shows the middle 90% of the data to filter out the significant outliers so there are no apparent outliers shown in the graph. 

*Fig 1.2.* The distribution of the hour listed is left-skewed with jobs in the data set mainly concentrated between 2:00 PM and 7:00 PM. Given the apparent skewness the center is the median hour listed at  around 4:00 PM. The IQR describing the spread of the 50% of the distribution is 4 hours, showing how the variability of the hour listed is relatively high. There are apparent outliers in this graph when the hour listed is around 12:00 am. 

*Fig 1.3.* The distribution of follower count is right-skewed with jobs in the data set having a generally lower number of followers.Given the apparent skewness the center is the median of followers of around 70 thousand followers. The IQR describing the spread of the 50% of the distribution is over 280,000 followers, which means the variability of follower count in the data set is relatively high.

```{r out.width="50%"}
#| echo: false
experience_plot <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary, y = per_applies, color = formatted_experience_level)) +
  geom_point() + 
  geom_smooth(method = 'lm',formula = y ~ x, se = F) +
  labs(x = "Hourly Maximum Salary",
       y = "Applications per view",
       color = "Experience Level",
       title = "Applications per view vs. maximum salary \nbased on experience level") + 
  facet_wrap(~formatted_experience_level) +
  guides(color = FALSE) 
experience_plot 
cor(linkedin_subset$hourly_max_salary, linkedin_subset$per_applies)
```

```{r}
#| echo: false
#| results: false
tapply(linkedin_subset$per_applies, linkedin_subset$formatted_experience_level, summary)
```

*Fig 2.* There is no apparent direction or shape between the hourly maximum salary and applications per view for each of the experience levels based on the graphs. The correlation between these two variables is around -0.033, indicating the relationship is moderately weak. The distribution of applications per view based on experience level and maximum salary is mostly concentrated when the applications per view is less than 0.50 and the hourly maximum salary is less than 200 for each experience level. The mid-senior level has apparent outliers when the hourly maximum salary is greater than 200 and the NA level has outliers when the applications per view is above 0.75. There is also not as much data for some of the experience levels, including internship and executive.

```{r out.width="50%"}
#| echo: false

follow_scat <- linkedin_subset |>
  filter(follower_count > 10000) |>
  ggplot(aes(x = follower_count, y = per_applies)) +
  geom_point(size = 0.3, color = "lightblue4") +
  labs(x = "Follower Count", y = "Applications per \nView (%)", title = "Applications per View\nvs. Follower Count")

salary_scat <- linkedin_subset |>
  ggplot(aes(x = hourly_max_salary, y = per_applies)) +
  geom_point(size = 0.3, color = "lightskyblue3") +
  labs(x = "Hourly Max Salary", y = "Applications per \nView (%)", title = "Applications per View\nvs. Max Hourly Salary")

(follow_scat + salary_scat) 
```

```{r}
#| echo: false
#group <- linkedin_subset |>
#  group_by(hour_listed) |>
#  summarise(mean_per_applies = mean(per_applies))
```

```{r}
#| echo: false
#group |>
#  ggplot(aes(x = hour_listed, y = mean_per_applies)) +
#  geom_col(fill = "lightblue3") +
#  labs(x = "Hour Listed (hours since 12 am)", y = "Applications per View (%)", title = "Applications per #View based on Hour of Day Listed")
```

*Fig 3.1.* There is a slight linear correlation between follower count of a company and percentage of viewers who apply to the job. Most of the observations are concentrated to have less than 1,000,000 followers, so it would be beneficial to remove outliers that have more followers than that.

*Fig 3.2.* There is a slight positive linear correlation between a job's adjusted hourly maximum salary and percentage of viewers who apply to the job. Most of the observations are concentrated to have less than \$200,000 for adjusted hourly max salary, so it would be beneficial to filter out outliers that are above this threshold. Additionally, it would be beneficial to remove the outlier where 100% of viewers applied to the job (per_applies = 1.0)

*Fig 3.3.* Convert to categorical variable, make a histogram that's colored by morning, afternoon, evening

*Fig 3.4.* This figure could be statistically misleading since the intervals are not consistent, and mean_per_applies is not a helpful response variable for the model. We should convert hour_listed to a categorical variable and create a pie chart instead to show the frequency of each level.

## Methodology

**Intro\
**The data provides information about job listing details such as views, applications, time posted, etc., as well as whether or not certain benefits were offered in the job listing. This information was scraped from LinkedIn on July 23 and 24, 2023.

**Joining Datasets**\
While most of our data is from the \`job_postings\` data set, we also wanted to include employee and follower count from the \`employee\` data set, and the type and number of benefits listed from the \`benefits\` data set. We joined all data sets together by \`company_id\`, and saved the data set as \`linkedin\`.

**Benefits**\
Then, we added the number of benefits to create a new "tot_benefits" predictor, which tells us the total number of benefits listed. If "remote_allowed" and "tot_benefits" had NA entries, we assumed the job did not allow remote work and did not list any benefits. Consequently, we imputed them to 0's. However, the majority of entries in 'benefits' are NA, which means no benefits were scraped from the listing. To make data from \`benefits\` a useful predictor, we created a new categorical variable\`'if_benefits\`, which tells us if any or no benefits are listed. If any benefits (i.e. paid maternity leave or 401k plan) were listed, then the post is considered \`listed\`, and otherwise considered \`none\`.

**Salary**\
We also made assumptions about other variables to normalize predictor variables. To compare salaries even if they were listed in different formats (such as hourly pay, monthly, or yearly salary), we normalized the variable using the categorical variable \`pay_period\`, which tells us if the job pays its worker the \`max_salary\` or \`min_salary\` amount, with hourly, monthly, and annual payments. We then calculated the hourly wage given the maximum pay for hourly, monthly, and yearly pay periods. We assumed 160 hours for the monthly payments (40 hour work week for 4 weeks), and 2080 hours for the annual payments (40 hour work week for 52 weeks). Then, we saved the new data in a variable called \`hourly_max_salary\`.

**Time Posted**\
Since the existing posted time is in different time zones, we converted the time format to EST time and only kept the hour. Then, we converted posted time from a quantitative variable to a categorical variable. Since it wouldn't make sense to have 24 different levels, we designated 4 levels: night (0 am - 5 am), morning (6 am - 11 am), afternoon (12 pm - 5 pm), and evening (6 pm - 11 pm).

**Drop NA**\
We dropped all \`NA\` values for all predictors we wanted to observe so we can keep our dataset consistent when testing different models. Specifically, we dropped NAs for \`hourly_max_salary\`, \`per_applies\`, \`follower_count\`, \`formatted_experience_level\`, \`original_listed_time\`, and \`remote_allowed\`.\
\
**Filtering**\
To remove significant outliers that may affect the model's precision, we filtered and only kept the middle 90% of the data for the "hourly_max_salary" and "follower_count" variables. To generalize our results to job listings with a reasonable number of views, we filtered out job listings with less than 5 views.

**Normalize Response Variable**\
Because each job has been listed for varying time durations, and its view count is directly related to its application count, we decided to normalize the application count with the view count. To do so, we created a new variable \`per_applies\`, which divides \`applications\` by \`views\`. We now use \`per_applies\` as our response variable.

**Model Type**\
Because \`per_applies\` is a numerical variable, a linear regression model would be most appropriate to predict the number of applications per view. As we addressed in the introduction, a person takes into consider many factors when applying to a job, so our model takes into consideration multiple predictors, including the hour of day posted, company follower count, experience level, maximum salary, ability to work remote, and if benefits are listed.

Checking interaction effect:

```{r out.width="50%"}
#| echo: false
linkedin_subset |>
  ggplot(aes(x = hourly_max_salary, y = per_applies, color = formatted_experience_level)) +
  geom_point(size = 1) +
  geom_smooth(method = "lm", formula = y~x, se = F) + 
  labs(
    x = "Hourly Max Salary (Dollars)",
    y = "Percent of Viewers that Applied",
    title = "Effect of Experience Level and Salary on
    Percent of Viewers that Applied",
    color = "Experience Level"
  )
```

To check for interaction effects, we look at the relationship between two variables when a variable is grouped to see if the percent of viewers that applied differs based on various categorical variables. We saw the potential for an interaction effect between the maximum hourly salary and the required experience level of the candidate for the position, as typically those with less experience are payed less. As seen in the figure below, as the maximum hourly salary increases, the percentage of applicants out of viewers increases at different rates between different levels of experience required. We also saw the potential for interaction effects between hourly maximum salary and if remote work was allowed; however, the best fit lines for there being no remote work and remote work parallel, indicating no potential relationship.

We split the \`linkedin\` dataset into training and testing data, with 75% of the data in training and 25% in testing. We then used cross-fold validation with 12 folds on the training data set to find the mean summary statistics (AIC,  BIC, Adjusted R-Squared) for each model and compared the different values to find the best possible model. This process was repeated for models containing each combination of predictor variables. We set a seed of (2) when splitting and folding the data to ensure reproducibility.

```{r split_data}
#| echo: false
set.seed(2)
linkedin_split <- initial_split(linkedin_subset)
linkedin_train <- training(linkedin_split)
linkedin_test <- testing(linkedin_split)
```

```{r cross_val}
#| echo: false
set.seed(2)
folds <- vfold_cv(linkedin_train, v = 12)
linkedin_spec <- linear_reg() |>
  set_engine("lm")
```

```{r recipe_1}
#| echo: false
set.seed(2)
linkedin_recipe1 <- recipe(per_applies ~ job_id + hourly_max_salary + follower_count + remote_allowed + formatted_experience_level + if_benefits + time_posted, data = linkedin_train) |>
  update_role(job_id, new_role = "ID") |> 
  step_mutate(follower_count = follower_count/1000000) |>
  step_center(hourly_max_salary, follower_count) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())
```

```{r recipe_2}
#| echo: false
set.seed(2)
linkedin_recipe2 <- recipe(per_applies ~ job_id + remote_allowed + 
                             formatted_experience_level + follower_count,
                           data = linkedin_train) |>
  update_role(job_id, new_role = "ID") |> 
  step_mutate(follower_count = follower_count/1000000) |>
  step_center(follower_count) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())
```

```{r}
#| echo: false
set.seed(2)
linkedin_recipe3 <- recipe(per_applies ~ job_id + formatted_experience_level + 
                             follower_count +
                            remote_allowed + hourly_max_salary, 
                           data = linkedin_train) |>
  step_string2factor(formatted_experience_level) |>
  step_interact(terms = ~ hourly_max_salary:formatted_experience_level) |>
  update_role(job_id, new_role = "ID") |> 
  step_mutate(follower_count = follower_count/1000000) |>
  step_center(hourly_max_salary, follower_count) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())
```

```{r workflow_1}
#| echo: false
set.seed(2)
linkedin_wflow1 <- workflow() |>
  add_recipe(linkedin_recipe1) |>
  add_model(linkedin_spec)

linkedin_wflow2 <- workflow() |>
  add_recipe(linkedin_recipe2) |>
  add_model(linkedin_spec)

linkedin_wflow3 <- workflow() |>
  add_recipe(linkedin_recipe3) |>
  add_model(linkedin_spec)
```

```{r}
#| echo: false
set.seed(2)
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

```{r resampled_fits}
#| echo: false
set.seed(2)
linkedin_fit_rs_full <- linkedin_wflow1 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
linkedin_fit_rs_red <- linkedin_wflow2 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))

linkedin_fit_rs_red_inter <- linkedin_wflow3 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
```

```{r resample_mean_stats}
#| echo: false
map_df(linkedin_fit_rs_full$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC)) |> 
  kable(digits = 3)
map_df(linkedin_fit_rs_red$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC)) |> 
  kable(digits = 3)
map_df(linkedin_fit_rs_red_inter$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC)) |> 
  kable(digits = 3)
```

```{r out.width = "50%"}
#| echo: false
linkedin_fit_full <- linkedin_wflow1 |>
  fit(data = linkedin_train)

tidy(linkedin_fit_full) |>
  kable(digits = 3)
```

The adjusted $R^2$ value for a model including all predictors (`hourly_max_salary`, `follower_count`, `remote_allowed`, `formatted_experience_level`, `hour_listed`, and `if_benefits`) is 0.0233, and the AIC and BIC was -2095.867 and -2021.562, respectively.

Using a significance level of $\alpha = 0.10$, `follower_count`, `remote_allowed`, if the job requires director level of experience, and if the job requires executive level of experience are the only statistically significant variables, with p-values of 0.025, about 0, 0.001, and 0.035 respectively.

Then, we made a reduced model with these statistically significant variables including `follower_count`, `remote_allowed`, and `formatted_experience_level` to compare models. The adjusted R\^2, AIC, and BIC were 0.0237, -2101.431, and -2053.664, respectively. Since this reduced model with less predictors has a higher adjusted $R^2$ and lower AIC and BIC, which all indicate a stronger or better model, we concluded that the reduced model with `follower_count`, `remote_allowed`, and `formatted_experience_level`as predictors works the better than the full model.

Lastly, we included a model with the variables from the reduced model, plus `hourly_max_salary` to explore the potential interaction effect between `hourly_max_salary` and `remote_allowed`, which we identified earlier has a potential interaction term. The adjusted $R^2$, AIC, and BIC are 0.0238, -2099.576, and -2041.193. Although the adjusted $R^2$ is higher for the reduced model with the interaction effect, the AIC and BIC were higher than those of the reduced model without the interaction effect, indicating that the reduced model without the interaction term is a stronger model. Since the adjusted $R^2$ for both models aren't significantly different, we decided that the reduced model without the interaction term would work better because of the lower AIC and BIC.

Overall, by comparing the adjusted $R^2$, AIC, and BIC values of each of the models (with all predictors, statistically significant predictors, and statistically significant predictors with an interaction effect), we concluded that the reduced model with `follower_count`, `remote_allowed`, and `formatted_experience_level` as predictors works as the best model.

## Results

```{r out.width="50%"}
#| echo: false
linkedin_red_fit <- linkedin_wflow2 |>
  fit(data = linkedin_train)

tidy(linkedin_red_fit) |>
  kable(digits = 3)
```

$per\_applies = 0.209 + 0.010(follower\_count(millions)) + 0.033(remote\_allowed)$ $- 0.054(Director) - 0.016(Entry\_level)-0.068(Executive)-0.003(Internship)$ $- 0.009(Mid\_Senior\_level)$

*Linearity:*

```{r out.width="50%"}
#| echo: false
linkedin_fit_red <- linear_reg() |>
  set_engine("lm") |>
  fit(per_applies ~ job_id + remote_allowed + 
                             formatted_experience_level + follower_count, data = linkedin_train)
```

```{r out.width="50%"}
#| echo: false
linkedin_red_aug <- augment(linkedin_fit_red$fit)
r1 <- ggplot(data = linkedin_red_aug, aes(x = .fitted, y = .resid)) +
geom_point(alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(x = "Predicted values", 
     y = "Residuals",
     title = "Residuals vs.\nPredicted Values")
r2 <- ggplot(data = linkedin_red_aug, aes(x = .fitted, y = follower_count)) +
geom_point(alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(x = "Predicted values", 
     y = "Follower Count",
     title = "Residuals vs.\nFollower Count")
r3 <- ggplot(data = linkedin_red_aug, aes(x = .fitted, y = remote_allowed)) +
geom_point(alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(x = "Predicted values", 
     y = "Remote Allowed",
     title = "Residuals vs.\nRemote")
r4 <- ggplot(data = linkedin_red_aug, aes(x = .fitted, y = formatted_experience_level)) +
geom_point(alpha = 0.7) +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(x = "Predicted values", 
     y = "Experience",
     title = "Residuals vs.\nExperiences")
r1+r2
r3+r4
```

Since the plot of the residuals vs. predicted values do not have a discernible pattern and the plots of the residuals vs. each predictor do not have a discernible pattern, linearity is met.

*Constant Variance:*

The vertical spread of the residuals is not constant in the plot of the residuals vs. predicted so the constant variance condition is not satisfied.

*Normality:*

```{r}
#| echo: false
norm <- ggplot(data = linkedin_red_aug, aes(x = .resid)) + 
  geom_histogram() + 
  labs(title = "Jobs vs Residuals",
       x = "Residuals")
```

The distribution of the residuals is unimodal but not symmetric, so the normality condition is not satisfied. However, the sample size is large enough to relax this condition since it is not satisfied.

*Independence:*

```{r out.width="50%"}
#| echo: false
ind <- ggplot(linkedin_red_aug, aes(y = .resid, x = 1:nrow(linkedin_red_aug))) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(x = "Order of data collection", 
       y = "Residuals",
       title = "Order of data collection 
       vs residuals")
```

Independence is not satisfied because we are looking at company-related attributes including the company follower count and the postings for different jobs from the same companies may be dependent of each other. Additionally, the dataset spans two days that are months apart so the observations within the same time period are not independent since the new job postings may be dependent from the older ones. However, there is clear pattern in the residuals vs. order of data collection plot.

```{r out.width="50%"}
#| echo: false
norm + ind
```

```{r}
#| echo: false
tidy(vif(linkedin_fit_red$fit))
```

Multicollinearity occurs where there are very high correlations among two or more predictor variables, and we need to check for multicollinearity because it causes a loss in precision in our estimates of the regression coefficients. All VIF values are less than 10, meaning multicolinearity is not a concern.

```{r}
#| echo: false
#| results: false
# training data
linkedin_train_pred <- predict(linkedin_red_fit, linkedin_train) |>
  bind_cols(linkedin_train)

rmse_train <- rmse(linkedin_train_pred, truth = per_applies, estimate = .pred)

# testing data
linkedin_test_pred <- predict(linkedin_red_fit, linkedin_test) |>
  bind_cols(linkedin_test)

rmse_test <- rmse(linkedin_test_pred, truth = per_applies, estimate = .pred)

bind_rows(rmse_train, rmse_test) |>
  kable(digits = 4)
```

The RMSE of the training data is 0.1189, and the RMSE of the testing data is 0.1193. Significantly lower RMSEs for training data compared to the testing data could be a sign of model overfit, which means that the model fits the training data too well where it doesn't model new unseen data well. However, since the RMSE values for the training and testing data are very close, this shows no evidence of model overfit.

```{r}
#| echo: false
stats <- glance(linkedin_red_fit) |>
 select(r.squared, adj.r.squared, AIC, BIC)
colnames(stats) <- c("R Squared", "Adj. R Squared", "AIC","BIC")
stats |>
 kable(digits = 3)
```

The low \$R\^2\$ value of 0.0280 suggests there is no significant relationship between the follower count, whether the job allows remote work, and the listed job experience level and the ratio of applications to views. Only 2.8% of the variation in the percent of viewers that applied is a result of the variation in the predictor variables: the follower count, whether the job allows remote, and the listed job experience level.

## Discussion + Conclusion

Our final model suggests that there is no statistically significant relationship between our response variable of the percent of viewers who applied and the maximum salary, number of followers, having a remote option, the job experience level, the hour at which it was posted, and if the benefits were posted. Our model suggests that the percentage of viewers who applied and the maximum salary, number of followers, having a remote option, the job experience level, the hour at which it was posted, and if the benefits were posted are not useful predictors and therefore do not provide much insight into what makes a LinkedIn post have significant popularity.

Some of the limitations of our analysis may be our treatment of NA data in creating \`if_benefits\` and \`remote_allowed\`. We assumed that since the data had not been included, it was not available in the post, and therefore the values could be inputted as 0. Another possible limitation is that we chose to normalize the maximum salaries across hourly, monthly, and yearly pay periods to create \`hourly_max_salary", and to do so needed to make assumptions about how often individuals would work. These assumptions could have been incorrect and thus impacted the accuracy of our final model.

Our limitation is also limited to the data that was scraped and gathered in the data set we used. Other possible influences to the percentage of applicants that was not scraped could be the number of shares the post received, the duration of the post, and if the location as if it was rural or not. There are also a lot of other less quantifiable variables that go into applying to a job, such as company culture and the company's mission statement, as well as societal influences such as current unemployment rates. To improve the prediction of the number of applicants as a percentage of viewers, we might have to consider a wider range of variables that were not available in the data sets we used in this project.

Some ways our analysis could be improved would be filtering out the outliers with a more precise method, such as limiting the views and applications to a certain range.

Potential issues relating to the data would include the time scraped from LinkedIn, since many of the original listed times and the listed times (the time it was scraped) are exactly the same, which is not meaningful in determining the number of applications over a certain amount of time. There were also many missing values in our data set which were filtered out, but there could be a skew in the remaining data that influenced our results.
